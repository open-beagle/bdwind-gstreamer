package webrtc

import (
	"context"
	"fmt"
	"log"
	"runtime"
	"sync"
	"sync/atomic"
	"time"
)

// PerformanceMonitor æ€§èƒ½ç›‘æ§å™¨
type PerformanceMonitor struct {
	// æ¶ˆæ¯å¤„ç†ç»Ÿè®¡
	messageStats    *MessageProcessingStats
	connectionStats *ConnectionStats
	systemStats     *SystemStats
	routingStats    *MessageRoutingStats

	// é…ç½®
	config *PerformanceMonitorConfig

	// æ§åˆ¶
	ctx    context.Context
	cancel context.CancelFunc
	mutex  sync.RWMutex

	// æŠ¥å‘Šç”Ÿæˆ
	reportInterval time.Duration
	lastReportTime time.Time

	// äº‹ä»¶å›è°ƒ
	alertCallbacks []func(alert *PerformanceAlert)
}

// PerformanceMonitorConfig æ€§èƒ½ç›‘æ§é…ç½®
type PerformanceMonitorConfig struct {
	// ç›‘æ§é—´éš”
	MonitorInterval time.Duration `json:"monitor_interval"`
	ReportInterval  time.Duration `json:"report_interval"`

	// é˜ˆå€¼é…ç½®
	SlowMessageThreshold    time.Duration `json:"slow_message_threshold"`
	HighMemoryThreshold     uint64        `json:"high_memory_threshold"`
	HighConnectionThreshold int           `json:"high_connection_threshold"`
	HighErrorRateThreshold  float64       `json:"high_error_rate_threshold"`

	// æ€§èƒ½ä¼˜åŒ–é…ç½®
	EnableConcurrentRouting bool `json:"enable_concurrent_routing"`
	MaxConcurrentMessages   int  `json:"max_concurrent_messages"`
	MessageBufferSize       int  `json:"message_buffer_size"`

	// ç»Ÿè®¡ä¿ç•™é…ç½®
	StatsRetentionPeriod time.Duration `json:"stats_retention_period"`
	DetailedStatsEnabled bool          `json:"detailed_stats_enabled"`
}

// MessageProcessingStats æ¶ˆæ¯å¤„ç†ç»Ÿè®¡
type MessageProcessingStats struct {
	// åŸºç¡€è®¡æ•°å™¨
	TotalMessages      int64 `json:"total_messages"`
	SuccessfulMessages int64 `json:"successful_messages"`
	FailedMessages     int64 `json:"failed_messages"`

	// æŒ‰æ¶ˆæ¯ç±»å‹ç»Ÿè®¡
	MessageTypeStats map[string]*MessageTypeStats `json:"message_type_stats"`

	// å»¶è¿Ÿç»Ÿè®¡
	AverageLatency time.Duration `json:"average_latency"`
	MinLatency     time.Duration `json:"min_latency"`
	MaxLatency     time.Duration `json:"max_latency"`
	P95Latency     time.Duration `json:"p95_latency"`
	P99Latency     time.Duration `json:"p99_latency"`

	// å»¶è¿Ÿåˆ†å¸ƒ
	LatencyBuckets map[string]int64 `json:"latency_buckets"`

	// æ—¶é—´çª—å£ç»Ÿè®¡
	LastMinuteStats *TimeWindowStats `json:"last_minute_stats"`
	LastHourStats   *TimeWindowStats `json:"last_hour_stats"`

	// æ…¢æ¶ˆæ¯ç»Ÿè®¡
	SlowMessages       int64                `json:"slow_messages"`
	SlowMessageDetails []*SlowMessageRecord `json:"slow_message_details"`

	mutex sync.RWMutex
}

// MessageTypeStats æŒ‰æ¶ˆæ¯ç±»å‹çš„ç»Ÿè®¡
type MessageTypeStats struct {
	Count          int64         `json:"count"`
	SuccessCount   int64         `json:"success_count"`
	ErrorCount     int64         `json:"error_count"`
	AverageLatency time.Duration `json:"average_latency"`
	TotalLatency   time.Duration `json:"total_latency"`
	MaxLatency     time.Duration `json:"max_latency"`
	LastProcessed  time.Time     `json:"last_processed"`
}

// TimeWindowStats æ—¶é—´çª—å£ç»Ÿè®¡
type TimeWindowStats struct {
	StartTime        time.Time     `json:"start_time"`
	EndTime          time.Time     `json:"end_time"`
	MessageCount     int64         `json:"message_count"`
	ErrorCount       int64         `json:"error_count"`
	AverageLatency   time.Duration `json:"average_latency"`
	ThroughputPerSec float64       `json:"throughput_per_sec"`
}

// SlowMessageRecord æ…¢æ¶ˆæ¯è®°å½•
type SlowMessageRecord struct {
	Timestamp      time.Time     `json:"timestamp"`
	ClientID       string        `json:"client_id"`
	MessageType    string        `json:"message_type"`
	ProcessingTime time.Duration `json:"processing_time"`
	Details        string        `json:"details"`
}

// ConnectionStats è¿æ¥ç»Ÿè®¡
type ConnectionStats struct {
	// è¿æ¥æ•°ç»Ÿè®¡
	TotalConnections  int64 `json:"total_connections"`
	ActiveConnections int64 `json:"active_connections"`
	PeakConnections   int64 `json:"peak_connections"`

	// è¿æ¥ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡
	AverageConnectionDuration time.Duration `json:"average_connection_duration"`
	ConnectionsPerMinute      float64       `json:"connections_per_minute"`
	DisconnectionsPerMinute   float64       `json:"disconnections_per_minute"`

	// è¿æ¥è´¨é‡ç»Ÿè®¡
	HealthyConnections   int64 `json:"healthy_connections"`
	UnhealthyConnections int64 `json:"unhealthy_connections"`
	ReconnectAttempts    int64 `json:"reconnect_attempts"`

	// æŒ‰åº”ç”¨ç»Ÿè®¡
	AppConnectionStats map[string]*AppConnectionStats `json:"app_connection_stats"`

	mutex sync.RWMutex
}

// AppConnectionStats æŒ‰åº”ç”¨çš„è¿æ¥ç»Ÿè®¡
type AppConnectionStats struct {
	AppName           string    `json:"app_name"`
	ActiveConnections int64     `json:"active_connections"`
	TotalConnections  int64     `json:"total_connections"`
	LastActivity      time.Time `json:"last_activity"`
}

// SystemStats ç³»ç»Ÿç»Ÿè®¡
type SystemStats struct {
	// å†…å­˜ä½¿ç”¨
	MemoryUsage *MemoryUsage `json:"memory_usage"`

	// CPUä½¿ç”¨
	CPUUsage float64 `json:"cpu_usage"`

	// Goroutineç»Ÿè®¡
	GoroutineCount int `json:"goroutine_count"`

	// GCç»Ÿè®¡
	GCStats *GCStats `json:"gc_stats"`

	// ç³»ç»Ÿè´Ÿè½½
	SystemLoad *SystemLoad `json:"system_load"`

	// æ›´æ–°æ—¶é—´
	LastUpdated time.Time `json:"last_updated"`

	mutex sync.RWMutex
}

// MemoryUsage å†…å­˜ä½¿ç”¨ç»Ÿè®¡
type MemoryUsage struct {
	AllocatedBytes      uint64  `json:"allocated_bytes"`
	TotalAllocatedBytes uint64  `json:"total_allocated_bytes"`
	SystemBytes         uint64  `json:"system_bytes"`
	HeapBytes           uint64  `json:"heap_bytes"`
	StackBytes          uint64  `json:"stack_bytes"`
	GCPauseTotal        uint64  `json:"gc_pause_total"`
	UsagePercentage     float64 `json:"usage_percentage"`
}

// GCStats åƒåœ¾å›æ”¶ç»Ÿè®¡
type GCStats struct {
	NumGC         uint32        `json:"num_gc"`
	PauseTotal    time.Duration `json:"pause_total"`
	LastPause     time.Duration `json:"last_pause"`
	AveragePause  time.Duration `json:"average_pause"`
	GCCPUFraction float64       `json:"gc_cpu_fraction"`
}

// SystemLoad ç³»ç»Ÿè´Ÿè½½
type SystemLoad struct {
	MessageQueueDepth   int     `json:"message_queue_depth"`
	ConnectionPoolUsage float64 `json:"connection_pool_usage"`
	ThreadPoolUsage     float64 `json:"thread_pool_usage"`
}

// MessageRoutingStats æ¶ˆæ¯è·¯ç”±ç»Ÿè®¡
type MessageRoutingStats struct {
	// è·¯ç”±æ€§èƒ½
	TotalRoutedMessages int64         `json:"total_routed_messages"`
	AverageRoutingTime  time.Duration `json:"average_routing_time"`
	RoutingErrors       int64         `json:"routing_errors"`

	// åè®®ç»Ÿè®¡
	ProtocolStats map[string]*ProtocolRoutingStats `json:"protocol_stats"`

	// å¹¶å‘ç»Ÿè®¡
	ConcurrentMessages int64 `json:"concurrent_messages"`
	MaxConcurrency     int64 `json:"max_concurrency"`
	QueuedMessages     int64 `json:"queued_messages"`

	mutex sync.RWMutex
}

// ProtocolRoutingStats åè®®è·¯ç”±ç»Ÿè®¡
type ProtocolRoutingStats struct {
	Protocol     string        `json:"protocol"`
	MessageCount int64         `json:"message_count"`
	AverageTime  time.Duration `json:"average_time"`
	ErrorCount   int64         `json:"error_count"`
	LastUsed     time.Time     `json:"last_used"`
}

// PerformanceAlert æ€§èƒ½è­¦æŠ¥
type PerformanceAlert struct {
	Type         string                 `json:"type"`
	Severity     string                 `json:"severity"`
	Message      string                 `json:"message"`
	Details      map[string]interface{} `json:"details"`
	Timestamp    time.Time              `json:"timestamp"`
	Threshold    interface{}            `json:"threshold"`
	CurrentValue interface{}            `json:"current_value"`
}

// DefaultPerformanceMonitorConfig é»˜è®¤æ€§èƒ½ç›‘æ§é…ç½®
func DefaultPerformanceMonitorConfig() *PerformanceMonitorConfig {
	return &PerformanceMonitorConfig{
		MonitorInterval:         5 * time.Second,
		ReportInterval:          60 * time.Second,
		SlowMessageThreshold:    1 * time.Second,
		HighMemoryThreshold:     1024 * 1024 * 1024, // 1GB
		HighConnectionThreshold: 1000,
		HighErrorRateThreshold:  0.05, // 5%
		EnableConcurrentRouting: true,
		MaxConcurrentMessages:   100,
		MessageBufferSize:       1000,
		StatsRetentionPeriod:    24 * time.Hour,
		DetailedStatsEnabled:    true,
	}
}

// NewPerformanceMonitor åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
func NewPerformanceMonitor(config *PerformanceMonitorConfig) *PerformanceMonitor {
	if config == nil {
		config = DefaultPerformanceMonitorConfig()
	}

	ctx, cancel := context.WithCancel(context.Background())

	pm := &PerformanceMonitor{
		messageStats: &MessageProcessingStats{
			MessageTypeStats:   make(map[string]*MessageTypeStats),
			LatencyBuckets:     make(map[string]int64),
			SlowMessageDetails: make([]*SlowMessageRecord, 0),
			LastMinuteStats:    &TimeWindowStats{StartTime: time.Now()},
			LastHourStats:      &TimeWindowStats{StartTime: time.Now()},
		},
		connectionStats: &ConnectionStats{
			AppConnectionStats: make(map[string]*AppConnectionStats),
		},
		systemStats: &SystemStats{
			MemoryUsage: &MemoryUsage{},
			GCStats:     &GCStats{},
			SystemLoad:  &SystemLoad{},
		},
		routingStats: &MessageRoutingStats{
			ProtocolStats: make(map[string]*ProtocolRoutingStats),
		},
		config:         config,
		ctx:            ctx,
		cancel:         cancel,
		reportInterval: config.ReportInterval,
		lastReportTime: time.Now(),
		alertCallbacks: make([]func(alert *PerformanceAlert), 0),
	}

	// åˆå§‹åŒ–å»¶è¿Ÿæ¡¶
	pm.initializeLatencyBuckets()

	return pm
}

// initializeLatencyBuckets åˆå§‹åŒ–å»¶è¿Ÿæ¡¶
func (pm *PerformanceMonitor) initializeLatencyBuckets() {
	buckets := []string{
		"0-1ms", "1-5ms", "5-10ms", "10-50ms", "50-100ms",
		"100-500ms", "500ms-1s", "1s-5s", "5s+",
	}

	pm.messageStats.mutex.Lock()
	defer pm.messageStats.mutex.Unlock()

	for _, bucket := range buckets {
		pm.messageStats.LatencyBuckets[bucket] = 0
	}
}

// Start å¯åŠ¨æ€§èƒ½ç›‘æ§
func (pm *PerformanceMonitor) Start() {
	log.Printf("ğŸš€ Starting performance monitor with interval %v", pm.config.MonitorInterval)

	// å¯åŠ¨ç›‘æ§åç¨‹
	go pm.monitorLoop()

	// å¯åŠ¨æŠ¥å‘Šåç¨‹
	go pm.reportLoop()

	log.Printf("âœ… Performance monitor started")
}

// Stop åœæ­¢æ€§èƒ½ç›‘æ§
func (pm *PerformanceMonitor) Stop() {
	log.Printf("ğŸ›‘ Stopping performance monitor...")
	pm.cancel()
	log.Printf("âœ… Performance monitor stopped")
}

// monitorLoop ç›‘æ§å¾ªç¯
func (pm *PerformanceMonitor) monitorLoop() {
	ticker := time.NewTicker(pm.config.MonitorInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			pm.collectSystemStats()
			pm.checkAlerts()
		case <-pm.ctx.Done():
			return
		}
	}
}

// reportLoop æŠ¥å‘Šå¾ªç¯
func (pm *PerformanceMonitor) reportLoop() {
	ticker := time.NewTicker(pm.reportInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			pm.generatePerformanceReport()
		case <-pm.ctx.Done():
			return
		}
	}
}

// RecordMessageProcessing è®°å½•æ¶ˆæ¯å¤„ç†
func (pm *PerformanceMonitor) RecordMessageProcessing(clientID, messageType string, processingTime time.Duration, success bool) {
	pm.messageStats.mutex.Lock()
	defer pm.messageStats.mutex.Unlock()

	// æ›´æ–°æ€»è®¡æ•°å™¨
	atomic.AddInt64(&pm.messageStats.TotalMessages, 1)
	if success {
		atomic.AddInt64(&pm.messageStats.SuccessfulMessages, 1)
	} else {
		atomic.AddInt64(&pm.messageStats.FailedMessages, 1)
	}

	// æ›´æ–°æ¶ˆæ¯ç±»å‹ç»Ÿè®¡
	if pm.messageStats.MessageTypeStats[messageType] == nil {
		pm.messageStats.MessageTypeStats[messageType] = &MessageTypeStats{}
	}

	typeStats := pm.messageStats.MessageTypeStats[messageType]
	typeStats.Count++
	if success {
		typeStats.SuccessCount++
	} else {
		typeStats.ErrorCount++
	}

	// æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
	typeStats.TotalLatency += processingTime
	typeStats.AverageLatency = time.Duration(int64(typeStats.TotalLatency) / typeStats.Count)
	if processingTime > typeStats.MaxLatency {
		typeStats.MaxLatency = processingTime
	}
	typeStats.LastProcessed = time.Now()

	// æ›´æ–°å…¨å±€å»¶è¿Ÿç»Ÿè®¡
	pm.updateLatencyStats(processingTime)

	// è®°å½•æ…¢æ¶ˆæ¯
	if processingTime > pm.config.SlowMessageThreshold {
		pm.recordSlowMessage(clientID, messageType, processingTime)
	}

	// æ›´æ–°æ—¶é—´çª—å£ç»Ÿè®¡
	pm.updateTimeWindowStats(processingTime, success)
}

// updateLatencyStats æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
func (pm *PerformanceMonitor) updateLatencyStats(latency time.Duration) {
	// æ›´æ–°å»¶è¿Ÿæ¡¶
	bucket := pm.getLatencyBucket(latency)
	pm.messageStats.LatencyBuckets[bucket]++

	// æ›´æ–°æœ€å°/æœ€å¤§å»¶è¿Ÿ
	if pm.messageStats.MinLatency == 0 || latency < pm.messageStats.MinLatency {
		pm.messageStats.MinLatency = latency
	}
	if latency > pm.messageStats.MaxLatency {
		pm.messageStats.MaxLatency = latency
	}
}

// getLatencyBucket è·å–å»¶è¿Ÿæ¡¶
func (pm *PerformanceMonitor) getLatencyBucket(latency time.Duration) string {
	ms := latency.Milliseconds()

	switch {
	case ms < 1:
		return "0-1ms"
	case ms < 5:
		return "1-5ms"
	case ms < 10:
		return "5-10ms"
	case ms < 50:
		return "10-50ms"
	case ms < 100:
		return "50-100ms"
	case ms < 500:
		return "100-500ms"
	case ms < 1000:
		return "500ms-1s"
	case ms < 5000:
		return "1s-5s"
	default:
		return "5s+"
	}
}

// recordSlowMessage è®°å½•æ…¢æ¶ˆæ¯
func (pm *PerformanceMonitor) recordSlowMessage(clientID, messageType string, processingTime time.Duration) {
	atomic.AddInt64(&pm.messageStats.SlowMessages, 1)

	record := &SlowMessageRecord{
		Timestamp:      time.Now(),
		ClientID:       clientID,
		MessageType:    messageType,
		ProcessingTime: processingTime,
		Details:        fmt.Sprintf("Processing time exceeded threshold of %v", pm.config.SlowMessageThreshold),
	}

	// é™åˆ¶æ…¢æ¶ˆæ¯è®°å½•æ•°é‡
	if len(pm.messageStats.SlowMessageDetails) >= 100 {
		pm.messageStats.SlowMessageDetails = pm.messageStats.SlowMessageDetails[1:]
	}
	pm.messageStats.SlowMessageDetails = append(pm.messageStats.SlowMessageDetails, record)

	log.Printf("âš ï¸ Slow message detected: client=%s, type=%s, time=%v", clientID, messageType, processingTime)
}

// updateTimeWindowStats æ›´æ–°æ—¶é—´çª—å£ç»Ÿè®¡
func (pm *PerformanceMonitor) updateTimeWindowStats(latency time.Duration, success bool) {
	now := time.Now()

	// æ›´æ–°æœ€è¿‘ä¸€åˆ†é’Ÿç»Ÿè®¡
	if now.Sub(pm.messageStats.LastMinuteStats.StartTime) > time.Minute {
		pm.messageStats.LastMinuteStats = &TimeWindowStats{StartTime: now}
	}
	pm.messageStats.LastMinuteStats.MessageCount++
	if !success {
		pm.messageStats.LastMinuteStats.ErrorCount++
	}

	// æ›´æ–°æœ€è¿‘ä¸€å°æ—¶ç»Ÿè®¡
	if now.Sub(pm.messageStats.LastHourStats.StartTime) > time.Hour {
		pm.messageStats.LastHourStats = &TimeWindowStats{StartTime: now}
	}
	pm.messageStats.LastHourStats.MessageCount++
	if !success {
		pm.messageStats.LastHourStats.ErrorCount++
	}
}

// RecordConnectionEvent è®°å½•è¿æ¥äº‹ä»¶
func (pm *PerformanceMonitor) RecordConnectionEvent(eventType string, clientID, appName string) {
	pm.connectionStats.mutex.Lock()
	defer pm.connectionStats.mutex.Unlock()

	switch eventType {
	case "connect":
		atomic.AddInt64(&pm.connectionStats.TotalConnections, 1)
		atomic.AddInt64(&pm.connectionStats.ActiveConnections, 1)

		// æ›´æ–°å³°å€¼è¿æ¥æ•°
		if pm.connectionStats.ActiveConnections > pm.connectionStats.PeakConnections {
			pm.connectionStats.PeakConnections = pm.connectionStats.ActiveConnections
		}

		// æ›´æ–°åº”ç”¨ç»Ÿè®¡
		if pm.connectionStats.AppConnectionStats[appName] == nil {
			pm.connectionStats.AppConnectionStats[appName] = &AppConnectionStats{
				AppName: appName,
			}
		}
		appStats := pm.connectionStats.AppConnectionStats[appName]
		appStats.ActiveConnections++
		appStats.TotalConnections++
		appStats.LastActivity = time.Now()

	case "disconnect":
		atomic.AddInt64(&pm.connectionStats.ActiveConnections, -1)

		// æ›´æ–°åº”ç”¨ç»Ÿè®¡
		if appStats, exists := pm.connectionStats.AppConnectionStats[appName]; exists {
			appStats.ActiveConnections--
			appStats.LastActivity = time.Now()
		}
	}
}

// RecordRoutingStats è®°å½•è·¯ç”±ç»Ÿè®¡
func (pm *PerformanceMonitor) RecordRoutingStats(protocol string, routingTime time.Duration, success bool) {
	pm.routingStats.mutex.Lock()
	defer pm.routingStats.mutex.Unlock()

	atomic.AddInt64(&pm.routingStats.TotalRoutedMessages, 1)
	if !success {
		atomic.AddInt64(&pm.routingStats.RoutingErrors, 1)
	}

	// æ›´æ–°åè®®ç»Ÿè®¡
	if pm.routingStats.ProtocolStats[protocol] == nil {
		pm.routingStats.ProtocolStats[protocol] = &ProtocolRoutingStats{
			Protocol: protocol,
		}
	}

	protocolStats := pm.routingStats.ProtocolStats[protocol]
	protocolStats.MessageCount++
	if !success {
		protocolStats.ErrorCount++
	}

	// æ›´æ–°å¹³å‡è·¯ç”±æ—¶é—´
	totalTime := time.Duration(int64(protocolStats.AverageTime) * (protocolStats.MessageCount - 1))
	protocolStats.AverageTime = (totalTime + routingTime) / time.Duration(protocolStats.MessageCount)
	protocolStats.LastUsed = time.Now()
}

// collectSystemStats æ”¶é›†ç³»ç»Ÿç»Ÿè®¡
func (pm *PerformanceMonitor) collectSystemStats() {
	pm.systemStats.mutex.Lock()
	defer pm.systemStats.mutex.Unlock()

	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)

	// æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡
	pm.systemStats.MemoryUsage.AllocatedBytes = memStats.Alloc
	pm.systemStats.MemoryUsage.TotalAllocatedBytes = memStats.TotalAlloc
	pm.systemStats.MemoryUsage.SystemBytes = memStats.Sys
	pm.systemStats.MemoryUsage.HeapBytes = memStats.HeapAlloc
	pm.systemStats.MemoryUsage.StackBytes = memStats.StackInuse
	pm.systemStats.MemoryUsage.GCPauseTotal = memStats.PauseTotalNs

	// è®¡ç®—å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
	if pm.config.HighMemoryThreshold > 0 {
		pm.systemStats.MemoryUsage.UsagePercentage = float64(memStats.Alloc) / float64(pm.config.HighMemoryThreshold) * 100
	}

	// æ›´æ–°GCç»Ÿè®¡
	pm.systemStats.GCStats.NumGC = memStats.NumGC
	pm.systemStats.GCStats.PauseTotal = time.Duration(memStats.PauseTotalNs)
	if memStats.NumGC > 0 {
		pm.systemStats.GCStats.LastPause = time.Duration(memStats.PauseNs[(memStats.NumGC+255)%256])
		pm.systemStats.GCStats.AveragePause = time.Duration(memStats.PauseTotalNs / uint64(memStats.NumGC))
	}
	pm.systemStats.GCStats.GCCPUFraction = memStats.GCCPUFraction

	// æ›´æ–°Goroutineæ•°é‡
	pm.systemStats.GoroutineCount = runtime.NumGoroutine()

	pm.systemStats.LastUpdated = time.Now()
}

// checkAlerts æ£€æŸ¥è­¦æŠ¥
func (pm *PerformanceMonitor) checkAlerts() {
	// æ£€æŸ¥å†…å­˜ä½¿ç”¨è­¦æŠ¥
	if pm.systemStats.MemoryUsage.UsagePercentage > 80 {
		pm.triggerAlert(&PerformanceAlert{
			Type:         "memory",
			Severity:     "warning",
			Message:      "High memory usage detected",
			Timestamp:    time.Now(),
			Threshold:    "80%",
			CurrentValue: pm.systemStats.MemoryUsage.UsagePercentage,
			Details: map[string]interface{}{
				"allocated_bytes": pm.systemStats.MemoryUsage.AllocatedBytes,
				"system_bytes":    pm.systemStats.MemoryUsage.SystemBytes,
			},
		})
	}

	// æ£€æŸ¥è¿æ¥æ•°è­¦æŠ¥
	if pm.connectionStats.ActiveConnections > int64(pm.config.HighConnectionThreshold) {
		pm.triggerAlert(&PerformanceAlert{
			Type:         "connections",
			Severity:     "warning",
			Message:      "High connection count detected",
			Timestamp:    time.Now(),
			Threshold:    pm.config.HighConnectionThreshold,
			CurrentValue: pm.connectionStats.ActiveConnections,
		})
	}

	// æ£€æŸ¥é”™è¯¯ç‡è­¦æŠ¥
	if pm.messageStats.TotalMessages > 0 {
		errorRate := float64(pm.messageStats.FailedMessages) / float64(pm.messageStats.TotalMessages)
		if errorRate > pm.config.HighErrorRateThreshold {
			pm.triggerAlert(&PerformanceAlert{
				Type:         "error_rate",
				Severity:     "critical",
				Message:      "High error rate detected",
				Timestamp:    time.Now(),
				Threshold:    pm.config.HighErrorRateThreshold,
				CurrentValue: errorRate,
				Details: map[string]interface{}{
					"total_messages":  pm.messageStats.TotalMessages,
					"failed_messages": pm.messageStats.FailedMessages,
				},
			})
		}
	}
}

// triggerAlert è§¦å‘è­¦æŠ¥
func (pm *PerformanceMonitor) triggerAlert(alert *PerformanceAlert) {
	log.Printf("ğŸš¨ Performance Alert: %s - %s (Current: %v, Threshold: %v)",
		alert.Type, alert.Message, alert.CurrentValue, alert.Threshold)

	// è°ƒç”¨è­¦æŠ¥å›è°ƒ
	for _, callback := range pm.alertCallbacks {
		go callback(alert)
	}
}

// AddAlertCallback æ·»åŠ è­¦æŠ¥å›è°ƒ
func (pm *PerformanceMonitor) AddAlertCallback(callback func(alert *PerformanceAlert)) {
	pm.mutex.Lock()
	defer pm.mutex.Unlock()
	pm.alertCallbacks = append(pm.alertCallbacks, callback)
}

// generatePerformanceReport ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
func (pm *PerformanceMonitor) generatePerformanceReport() {
	now := time.Now()
	duration := now.Sub(pm.lastReportTime)

	log.Printf("ğŸ“Š === Performance Report (Period: %v) ===", duration)

	// æ¶ˆæ¯å¤„ç†ç»Ÿè®¡
	pm.messageStats.mutex.RLock()
	log.Printf("ğŸ“¨ Message Processing:")
	log.Printf("  Total Messages: %d", pm.messageStats.TotalMessages)
	log.Printf("  Success Rate: %.2f%%", float64(pm.messageStats.SuccessfulMessages)/float64(pm.messageStats.TotalMessages)*100)
	log.Printf("  Average Latency: %v", pm.messageStats.AverageLatency)
	log.Printf("  Slow Messages: %d", pm.messageStats.SlowMessages)

	// æŒ‰æ¶ˆæ¯ç±»å‹ç»Ÿè®¡
	log.Printf("ğŸ“‹ Message Types:")
	for msgType, stats := range pm.messageStats.MessageTypeStats {
		successRate := float64(stats.SuccessCount) / float64(stats.Count) * 100
		log.Printf("  %s: %d messages, %.2f%% success, avg latency: %v",
			msgType, stats.Count, successRate, stats.AverageLatency)
	}
	pm.messageStats.mutex.RUnlock()

	// è¿æ¥ç»Ÿè®¡
	pm.connectionStats.mutex.RLock()
	log.Printf("ğŸ”— Connections:")
	log.Printf("  Active: %d, Total: %d, Peak: %d",
		pm.connectionStats.ActiveConnections,
		pm.connectionStats.TotalConnections,
		pm.connectionStats.PeakConnections)

	// æŒ‰åº”ç”¨ç»Ÿè®¡
	log.Printf("ğŸ“± Applications:")
	for appName, stats := range pm.connectionStats.AppConnectionStats {
		log.Printf("  %s: %d active, %d total", appName, stats.ActiveConnections, stats.TotalConnections)
	}
	pm.connectionStats.mutex.RUnlock()

	// ç³»ç»Ÿç»Ÿè®¡
	pm.systemStats.mutex.RLock()
	log.Printf("ğŸ’» System:")
	log.Printf("  Memory: %.2f%% (%.2f MB allocated)",
		pm.systemStats.MemoryUsage.UsagePercentage,
		float64(pm.systemStats.MemoryUsage.AllocatedBytes)/1024/1024)
	log.Printf("  Goroutines: %d", pm.systemStats.GoroutineCount)
	log.Printf("  GC: %d collections, %.2f%% CPU",
		pm.systemStats.GCStats.NumGC,
		pm.systemStats.GCStats.GCCPUFraction*100)
	pm.systemStats.mutex.RUnlock()

	// è·¯ç”±ç»Ÿè®¡
	pm.routingStats.mutex.RLock()
	log.Printf("ğŸš¦ Message Routing:")
	log.Printf("  Total Routed: %d", pm.routingStats.TotalRoutedMessages)
	log.Printf("  Routing Errors: %d", pm.routingStats.RoutingErrors)
	log.Printf("  Average Routing Time: %v", pm.routingStats.AverageRoutingTime)
	pm.routingStats.mutex.RUnlock()

	log.Printf("ğŸ“Š === End Performance Report ===")

	pm.lastReportTime = now
}

// GetStats è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
func (pm *PerformanceMonitor) GetStats() map[string]interface{} {
	pm.mutex.RLock()
	defer pm.mutex.RUnlock()

	return map[string]interface{}{
		"message_stats":    pm.messageStats,
		"connection_stats": pm.connectionStats,
		"system_stats":     pm.systemStats,
		"routing_stats":    pm.routingStats,
		"config":           pm.config,
		"last_report_time": pm.lastReportTime,
	}
}

// GetMessageStats è·å–æ¶ˆæ¯ç»Ÿè®¡
func (pm *PerformanceMonitor) GetMessageStats() *MessageProcessingStats {
	pm.messageStats.mutex.RLock()
	defer pm.messageStats.mutex.RUnlock()

	// è¿”å›å‰¯æœ¬ä»¥é¿å…å¹¶å‘è®¿é—®é—®é¢˜
	stats := *pm.messageStats
	return &stats
}

// GetConnectionStats è·å–è¿æ¥ç»Ÿè®¡
func (pm *PerformanceMonitor) GetConnectionStats() *ConnectionStats {
	pm.connectionStats.mutex.RLock()
	defer pm.connectionStats.mutex.RUnlock()

	stats := *pm.connectionStats
	return &stats
}

// GetSystemStats è·å–ç³»ç»Ÿç»Ÿè®¡
func (pm *PerformanceMonitor) GetSystemStats() *SystemStats {
	pm.systemStats.mutex.RLock()
	defer pm.systemStats.mutex.RUnlock()

	stats := *pm.systemStats
	return &stats
}

// ResetStats é‡ç½®ç»Ÿè®¡ä¿¡æ¯
func (pm *PerformanceMonitor) ResetStats() {
	pm.mutex.Lock()
	defer pm.mutex.Unlock()

	log.Printf("ğŸ”„ Resetting performance statistics")

	// é‡ç½®æ¶ˆæ¯ç»Ÿè®¡
	pm.messageStats.mutex.Lock()
	pm.messageStats.TotalMessages = 0
	pm.messageStats.SuccessfulMessages = 0
	pm.messageStats.FailedMessages = 0
	pm.messageStats.SlowMessages = 0
	pm.messageStats.MessageTypeStats = make(map[string]*MessageTypeStats)
	pm.messageStats.SlowMessageDetails = make([]*SlowMessageRecord, 0)
	pm.initializeLatencyBuckets()
	pm.messageStats.mutex.Unlock()

	// é‡ç½®è·¯ç”±ç»Ÿè®¡
	pm.routingStats.mutex.Lock()
	pm.routingStats.TotalRoutedMessages = 0
	pm.routingStats.RoutingErrors = 0
	pm.routingStats.ProtocolStats = make(map[string]*ProtocolRoutingStats)
	pm.routingStats.mutex.Unlock()

	log.Printf("âœ… Performance statistics reset completed")
}
